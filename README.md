# ğŸ¤– Chipade AI Chatbot Collection

A comprehensive collection of AI chatbots built with different technologies including LangChain, Ollama, Groq, and Hugging Face. Features both CLI and beautiful Streamlit web interfaces.

## ğŸŒŸ Project Overview

This repository contains multiple AI chatbot implementations:

1. **Ollama Chatbot** - Fully local AI assistant using Ollama (Llama 3)
2. **Streamlit Ollama Chatbot** - Stunning web UI for Ollama chatbot
3. **Groq Chatbot** - Cloud-based chatbot using Groq's LLaMA models
4. **Hugging Face Chatbot** - Integration with Hugging Face models
5. **Simple CLI Chatbot** - Basic command-line chatbot



### Ollama Chatbot (Local & Private)
- ğŸ”’ **100% Private**: Runs completely on your local machine
- ğŸ’¬ **Context-Aware**: Maintains conversation history
- âš¡ **Real-time Streaming**: See responses appear word by word
- ï¿½ **Auto-Save**: Conversations saved to JSON files
- ğŸ¯ **Adaptive AI**: Adjusts response style based on context


## ğŸš€ Installation

### Prerequisites
- Python 3.8+
- Ollama installed locally (for Ollama chatbots)
- API keys (for Groq/Hugging Face)

### Step 1: Clone the Repository
```bash
git clone https://github.com/codewithyasho/Chipade-AiChatbot.git
cd Chipade-AiChatbot
```

### Step 2: Install Dependencies
```bash
pip install -r requirements.txt
```

### Step 3: Set Up Ollama (for Local Chatbot)
1. Install Ollama from [ollama.ai](https://ollama.ai)
2. Pull the Llama model:
```bash
ollama pull llama3.2:3b
```

### Step 4: Configure API Keys (Optional)
Create a `.env` file for cloud-based chatbots:
```env
GROQ_API_KEY=your_groq_api_key_here
HUGGINGFACE_API_KEY=your_hf_api_key_here
```

## ğŸ¯ Usage

### Ollama Chatbot (CLI)
Run the command-line version:
```bash
python Ollama-chatbot.py
```

Features:
- Type your messages and get instant AI responses
- Conversation history maintained throughout session
- Type `exit`, `quit`, or `bye` to end
- Chat history auto-saved to `ollama-chat-history.json`

### Streamlit Ollama Chatbot (Web UI) â­ **RECOMMENDED**
Launch the beautiful web interface:
```bash
streamlit run streamlit_ollama_chatbot.py
```

Then open your browser at `http://localhost:8501`

**Web UI Features:**
- ğŸ¨ Stunning gradient purple design
- ğŸŒ¡ï¸ Temperature slider (0.0 - 1.0) to control creativity
- ğŸ“Š Real-time message statistics
- ğŸ’¾ Save conversations with timestamps
- ğŸ—‘ï¸ Quick clear chat button
- ğŸ¤– Smooth message streaming animations

### Groq Chatbot
```bash
python app.py
# or
streamlit run streamlit_chatbot.py
```

### Simple CLI Chatbot
```bash
python Simple-Chatbot.py
```

## ğŸ“ Project Structure

```
project1/
â”œâ”€â”€ streamlit_ollama_chatbot.py  # â­ Main Streamlit web UI (Ollama)
â”œâ”€â”€ Ollama-chatbot.py            # CLI Ollama chatbot
â”œâ”€â”€ app.py                       # Groq chatbot app
â”œâ”€â”€ Simple-Chatbot.py            # Basic CLI chatbot
â”œâ”€â”€ ollama.ipynb                 # Ollama experiments notebook
â”œâ”€â”€ langchain-groq.ipynb         # Groq integration notebook
â”œâ”€â”€ huggingface.ipynb            # Hugging Face notebook
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ .env                         # API keys (create this)
â””â”€â”€ README.md                    # This file
```

## ğŸ› ï¸ Technologies Used

### Core Technologies
- **Python 3.8+**: Primary language
- **LangChain**: LLM orchestration and conversation management
- **Streamlit**: Modern web UI framework

### AI Models & Platforms
- **Ollama**: Local LLM runtime (Llama 3.2:3b)
- **Groq**: Cloud-based LLM API (LLaMA 3.3)
- **Hugging Face**: Model hub integration

### Additional Libraries
- **langchain-ollama**: Ollama integration for LangChain
- **langchain-groq**: Groq integration for LangChain
- **langchain-core**: Core LangChain components
- **python-dotenv**: Environment variable management
- **reportlab**: PDF generation for chat exports


## ğŸ”’ Privacy & Security

**Ollama Chatbot Benefits:**
- âœ… Runs 100% locally on your machine
- âœ… No data sent to external servers
- âœ… Complete privacy and control
- âœ… Works offline (after model download)
- âœ… No API costs or rate limits

## ğŸ“ Configuration

### Adjusting Model Settings

In the code, you can modify:

```python
llm = ChatOllama(
    model="llama3.2:3b",      # Change model
    temperature=0.7,           # Creativity (0.0-1.0)
    verbose=False,             # Debug output
    num_thread=10              # CPU threads
)
```

### Available Ollama Models
- `llama3.2:3b` - Fast, efficient (recommended)
- `llama3:8b` - More capable, slower
- `llama3:70b` - Most capable (requires powerful hardware)

Pull models with: `ollama pull <model-name>`

## ğŸ› Troubleshooting

### Ollama Connection Error
**Problem**: "Error: Ollama connection failed"
**Solution**: 
1. Make sure Ollama is running: `ollama serve`
2. Verify model is downloaded: `ollama list`
3. Pull model if needed: `ollama pull llama3.2:3b`

### Import Errors
**Problem**: "ModuleNotFoundError"
**Solution**: Install dependencies:
```bash
pip install -r requirements.txt
```

### Streamlit Not Found
**Problem**: "streamlit: command not found"
**Solution**: 
```bash
pip install streamlit
```

## ğŸ¤ Contributing

Contributions are welcome! Feel free to:
- Report bugs
- Suggest new features
- Submit pull requests
- Improve documentation

## ğŸ“„ License

This project is open source and available for educational purposes.

## ğŸ‘¤ Author

**Yasho Chipade** ([@codewithyasho](https://github.com/codewithyasho))

## ğŸ™ Acknowledgments

- **Ollama** - For making local LLMs accessible
- **LangChain** - For excellent LLM orchestration tools
- **Streamlit** - For the amazing web framework
- **Meta AI** - For the Llama models

---

<div align="center">

**Made with â¤ï¸ using Streamlit & Ollama**

ğŸ”’ **100% Private & Local** | âš¡ **Fast & Efficient** | ğŸ¨ **Beautiful UI**

[Report Bug](https://github.com/codewithyasho/Chipade-AiChatbot/issues) Â· [Request Feature](https://github.com/codewithyasho/Chipade-AiChatbot/issues)

</div>
